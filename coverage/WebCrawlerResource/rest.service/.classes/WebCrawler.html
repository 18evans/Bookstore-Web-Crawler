<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html id="htmlId">
<head>
    <title>Coverage Report :: WebCrawler</title>
    <style type="text/css">
        @import "../../.css/coverage.css";
    </style>
</head>

<body>
<div class="header"></div>

<div class="content">
    <div class="breadCrumbs">
        [ <a href="../../index.html">all classes</a> ]
        [ <a href="../index.html">rest.service</a> ]
    </div>

    <h1>Coverage Summary for Class: WebCrawler (rest.service)</h1>

    <table class="coverageStats">
        <tr>
            <th class="name">Class</th>
            <th class="coverageStat
">
                Class, %
            </th>
            <th class="coverageStat
">
                Method, %
            </th>
            <th class="coverageStat
">
                Line, %
            </th>
        </tr>
        <tr>
            <td class="name">WebCrawler</td>
            <td class="coverageStat">
  <span class="percent">
    100%
  </span>
                <span class="absValue">
    (1/ 1)
  </span>
            </td>
            <td class="coverageStat">
  <span class="percent">
    100%
  </span>
                <span class="absValue">
    (15/ 15)
  </span>
            </td>
            <td class="coverageStat">
  <span class="percent">
    98.4%
  </span>
                <span class="absValue">
    (61/ 62)
  </span>
            </td>
        </tr>

    </table>

    <br/>
    <br/>


    <div class="sourceCode"><i>1</i>&nbsp;package rest.service;
        <i>2</i>&nbsp;
        <i>3</i>&nbsp;import org.apache.commons.lang3.StringUtils;
        <i>4</i>&nbsp;import org.jsoup.Jsoup;
        <i>5</i>&nbsp;import org.jsoup.nodes.Document;
        <i>6</i>&nbsp;import org.jsoup.nodes.Element;
        <i>7</i>&nbsp;import org.jsoup.select.Elements;
        <i>8</i>&nbsp;import rest.service.model.Item;
        <i>9</i>&nbsp;
        <i>10</i>&nbsp;import java.io.IOException;
        <i>11</i>&nbsp;import java.net.URL;
        <i>12</i>&nbsp;import java.util.Collections;
        <i>13</i>&nbsp;import java.util.HashSet;
        <i>14</i>&nbsp;import java.util.Set;
        <i>15</i>&nbsp;
        <i>16</i>&nbsp;public class WebCrawler {
        <i>17</i>&nbsp;
        <i>18</i>&nbsp; private Statistic statistic;
        <i>19</i>&nbsp; private final URL url;
        <b class="fc"><i>20</i>&nbsp; private Scraper scraper = new Scraper();</b>
        <i>21</i>&nbsp; private final Set&lt;URL&gt; exploredUrls;
        <i>22</i>&nbsp; private final Set&lt;URL&gt; initialToBeExploredUrls;
        <i>23</i>&nbsp; private final Set&lt;Item&gt; foundItems;
        <i>24</i>&nbsp;
        <b class="fc"><i>25</i>&nbsp; public WebCrawler(URL url, Item type, String keyword) {</b>
        <b class="fc"><i>26</i>&nbsp; this.url = url;</b>
        <b class="fc"><i>27</i>&nbsp; initialToBeExploredUrls = Collections.singleton(url);</b>
        <b class="fc"><i>28</i>&nbsp; statistic = new Statistic(type, keyword);</b>
        <b class="fc"><i>29</i>&nbsp; exploredUrls = new HashSet&lt;&gt;();</b>
        <b class="fc"><i>30</i>&nbsp; foundItems = new HashSet&lt;&gt;();</b>
        <b class="fc"><i>31</i>&nbsp; }</b>
        <i>32</i>&nbsp;
        <i>33</i>&nbsp; /***
        <i>34</i>&nbsp; * Set the Statistic object - for mocking purpose
        <i>35</i>&nbsp; * @param statistic
        <i>36</i>&nbsp; */
        <i>37</i>&nbsp; public void setStatistic(Statistic statistic) {
        <b class="fc"><i>38</i>&nbsp; this.statistic = statistic;</b>
        <b class="fc"><i>39</i>&nbsp; }</b>
        <i>40</i>&nbsp;
        <i>41</i>&nbsp; /**
        <i>42</i>&nbsp; * Method calls private crawl which will recursively go through the URL,
        <i>43</i>&nbsp; * looking for the specified objects either by using BFS or DFS searching algorithm.
        <i>44</i>&nbsp; *
        <i>45</i>&nbsp; * @return A Set Collection of the Item
        <i>46</i>&nbsp; */
        <i>47</i>&nbsp; public Set&lt;Item&gt; startCrawler() {
        <b class="fc"><i>48</i>&nbsp; return crawl(initialToBeExploredUrls);</b>
        <i>49</i>&nbsp; }
        <i>50</i>&nbsp;
        <i>51</i>&nbsp; /**
        <i>52</i>&nbsp; * Method wil recursively go through the passed collection of url parameter,
        <i>53</i>&nbsp; * until it reaches the end recursively adding it&#39;s finding to its return type of
        <i>54</i>&nbsp; * a Set of Item objects.
        <i>55</i>&nbsp; *
        <i>56</i>&nbsp; * @param urls the url&#39;s left to go through
        <i>57</i>&nbsp; * @return the found Items
        <i>58</i>&nbsp; */
        <i>59</i>&nbsp; private Set&lt;Item&gt; crawl(Set&lt;URL&gt; urls) {
        <b class="fc"><i>60</i>&nbsp; urls.removeAll(this.exploredUrls);</b>
        <b class="fc"><i>61</i>&nbsp; if (!urls.isEmpty()) {</b>
        <b class="fc"><i>62</i>&nbsp; this.exploredUrls.addAll(urls);</b>
        <b class="fc"><i>63</i>&nbsp; final Set&lt;URL&gt; newUrls = new HashSet&lt;&gt;();</b>
        <i>64</i>&nbsp; try {
        <b class="fc"><i>65</i>&nbsp; for (final URL url : urls) {</b>
        <b class="fc"><i>66</i>&nbsp; this.statistic.increasePagesExplored();</b>
        <b class="fc"><i>67</i>&nbsp; final Document document = Jsoup.connect(url.toString()).get();</b>
        <b class="fc"><i>68</i>&nbsp; final Elements urlsOnPage = document.select(&quot;a[href]&quot;);</b>
        <b class="fc"><i>69</i>&nbsp; Item newFoundItem = scraper.scrapeAndGetItem(document);</b>
        <b class="fc"><i>70</i>&nbsp; if (newFoundItem != null &amp;&amp;</b>
        <b class="fc"><i>71</i>&nbsp; (getItem() == null || newFoundItem.getClass().equals(getItem().getClass())) &amp;&amp;
            //if Type filtering is set check if Type is same as the set one</b>
        <b class="fc"><i>72</i>&nbsp; foundItems.stream().noneMatch(o -&gt; o.compareTo(newFoundItem))) { //do not add
            if no new found element or an element with same properties exists</b>
        <b class="fc"><i>73</i>&nbsp; foundItems.add(newFoundItem);</b>
        <i>74</i>&nbsp; }
        <b class="fc"><i>75</i>&nbsp; for (final Element element : urlsOnPage) {</b>
        <b class="fc"><i>76</i>&nbsp; final String urlText = element.attr(&quot;abs:href&quot;);</b>
        <b class="fc"><i>77</i>&nbsp; final URL discoveredUrl = new URL(urlText);</b>
        <b class="fc"><i>78</i>&nbsp; if (discoveredUrl.getHost().startsWith(getInitUrl().getHost())) { // limit future
            URL crawling only within the initial host</b>
        <b class="fc"><i>79</i>&nbsp; newUrls.add(discoveredUrl);</b>
        <i>80</i>&nbsp; }
        <b class="fc"><i>81</i>&nbsp; }</b>
        <b class="fc"><i>82</i>&nbsp; }</b>
        <b class="nc"><i>83</i>&nbsp; } catch (IOException ex) {</b>
        <i>84</i>&nbsp;
        <b class="fc"><i>85</i>&nbsp; }</b>
        <b class="fc"><i>86</i>&nbsp; this.statistic.increaseSearchDepth();</b>
        <b class="fc"><i>87</i>&nbsp; return crawl(newUrls);</b>
        <i>88</i>&nbsp; } else {
        <i>89</i>&nbsp; // filter all the item founds to match the searching criteria
        <b class="fc"><i>90</i>&nbsp; return process(foundItems);</b>
        <i>91</i>&nbsp; }
        <i>92</i>&nbsp; }
        <i>93</i>&nbsp;
        <i>94</i>&nbsp; /***
        <i>95</i>&nbsp; * Filter out all un-matched item and return the collection of close-matched items
        <i>96</i>&nbsp; * @param foundItems - the raw result collection
        <i>97</i>&nbsp; * @return
        <i>98</i>&nbsp; */
        <i>99</i>&nbsp; private Set&lt;Item&gt; process(Set&lt;Item&gt; foundItems) {
        <b class="fc"><i>100</i>&nbsp; Set&lt;Item&gt; result = new HashSet&lt;&gt;();</b>
        <b class="fc"><i>101</i>&nbsp; String targetKeyword = this.statistic.getKeyword();</b>
        <i>102</i>&nbsp;
        <b class="fc"><i>103</i>&nbsp; for (Item i : foundItems) {</b>
        <b class="fc"><i>104</i>&nbsp; if (StringUtils.isBlank(targetKeyword) ||</b>
        <b class="fc"><i>105</i>&nbsp; i.getTitle().contains(targetKeyword) ||</b>
        <b class="fc"><i>106</i>&nbsp; i.getGenre().contains(targetKeyword) ||</b>
        <b class="fc"><i>107</i>&nbsp; i.getYear().toString().contains(targetKeyword) ||</b>
        <b class="fc"><i>108</i>&nbsp; i.getFormat().contains(targetKeyword)) {</b>
        <b class="fc"><i>109</i>&nbsp; result.add(i);</b>
        <i>110</i>&nbsp; }
        <b class="fc"><i>111</i>&nbsp; }</b>
        <i>112</i>&nbsp;
        <b class="fc"><i>113</i>&nbsp; return result;</b>
        <i>114</i>&nbsp; }
        <i>115</i>&nbsp;
        <i>116</i>&nbsp; /***
        <i>117</i>&nbsp; * Return the initial url
        <i>118</i>&nbsp; * @return
        <i>119</i>&nbsp; */
        <i>120</i>&nbsp; public URL getInitUrl() {
        <b class="fc"><i>121</i>&nbsp; return this.url;</b>
        <i>122</i>&nbsp; }
        <i>123</i>&nbsp;
        <i>124</i>&nbsp; /***
        <i>125</i>&nbsp; * return the keyword
        <i>126</i>&nbsp; * @return
        <i>127</i>&nbsp; */
        <i>128</i>&nbsp; public String getKeyword() {
        <b class="fc"><i>129</i>&nbsp; return statistic.getKeyword();</b>
        <i>130</i>&nbsp; }
        <i>131</i>&nbsp;
        <i>132</i>&nbsp; public Item getItem() {
        <b class="fc"><i>133</i>&nbsp; return statistic.getType();</b>
        <i>134</i>&nbsp; }
        <i>135</i>&nbsp;
        <i>136</i>&nbsp; public Statistic getStatistic() {
        <b class="fc"><i>137</i>&nbsp; return statistic;</b>
        <i>138</i>&nbsp; }
        <i>139</i>&nbsp;
        <i>140</i>&nbsp; public Set&lt;URL&gt; getExploredUrls() {
        <b class="fc"><i>141</i>&nbsp; return exploredUrls;</b>
        <i>142</i>&nbsp; }
        <i>143</i>&nbsp;
        <i>144</i>&nbsp; public Set&lt;URL&gt; getInitialToBeExploredUrls() {
        <b class="fc"><i>145</i>&nbsp; return initialToBeExploredUrls;</b>
        <i>146</i>&nbsp; }
        <i>147</i>&nbsp;
        <i>148</i>&nbsp; /***
        <i>149</i>&nbsp; * Set the Scraper object - for mocking purpose
        <i>150</i>&nbsp; * @param scraper
        <i>151</i>&nbsp; */
        <i>152</i>&nbsp; public void setScraper(Scraper scraper) {
        <b class="fc"><i>153</i>&nbsp; this.scraper = scraper;</b>
        <b class="fc"><i>154</i>&nbsp; }</b>
        <i>155</i>&nbsp;
        <i>156</i>&nbsp; /***
        <i>157</i>&nbsp; * Change the target keyword and reset the statistic to start a new crawling process.
        <i>158</i>&nbsp; * @param newKeyword
        <i>159</i>&nbsp; */
        <i>160</i>&nbsp; public void changeKeyword(String newKeyword) {
        <b class="fc"><i>161</i>&nbsp; this.statistic.changeKeyword(newKeyword);</b>
        <b class="fc"><i>162</i>&nbsp; this.statistic.resetData();</b>
        <b class="fc"><i>163</i>&nbsp; }</b>
        <i>164</i>&nbsp;
        <i>165</i>&nbsp; /***
        <i>166</i>&nbsp; * Change the target type and reset the statistic to start a new crawling process
        <i>167</i>&nbsp; * @param newType
        <i>168</i>&nbsp; */
        <i>169</i>&nbsp; public void changeType(Item newType) {
        <b class="fc"><i>170</i>&nbsp; this.statistic.changeTargetType(newType);</b>
        <b class="fc"><i>171</i>&nbsp; this.statistic.resetData();</b>
        <b class="fc"><i>172</i>&nbsp; }</b>
        <i>173</i>&nbsp;
        <i>174</i>&nbsp; /***
        <i>175</i>&nbsp; * Retunr the current search depth after the crawling process already finished
        <i>176</i>&nbsp; * @return
        <i>177</i>&nbsp; */
        <i>178</i>&nbsp; public Integer getSearchDepth() {
        <b class="fc"><i>179</i>&nbsp; return this.statistic.getSearchDepth();</b>
        <i>180</i>&nbsp; }
        <i>181</i>&nbsp;}
    </div>
</div>

<div class="footer">

    <div style="float:right;">generated on 2019-06-19 07:37</div>
</div>
</body>
</html>
